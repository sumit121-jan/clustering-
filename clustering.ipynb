{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theory quesitons"
      ],
      "metadata": {
        "id": "j56oA4CIjVxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**1. What is unsupervised learning in the context of machine learning?**\n",
        "Unsupervised learning is a type of machine learning where the model is given input data without labeled responses. The goal is to discover hidden patterns, structures, or groupings in the data, with clustering being one of the primary tasks.\n",
        "\n",
        "**2. How does K-Means clustering algorithm work?**\n",
        "K-Means starts by initializing `k` centroids. It assigns each data point to the nearest centroid, forms clusters, and then recalculates the centroids as the mean of the points in each cluster. This process is repeated until the centroids stabilize.\n",
        "\n",
        "**3. Explain the concept of a dendrogram in hierarchical clustering.**\n",
        "A dendrogram is a tree-like diagram used to visualize the merging (or splitting) of clusters in hierarchical clustering. It helps determine the number of clusters by cutting the tree at a desired height.\n",
        "\n",
        "**4. What is the main difference between K-Means and Hierarchical Clustering?**\n",
        "K-Means is a partitional algorithm that requires the number of clusters as input and assigns data points based on distance to centroids. Hierarchical clustering builds a hierarchy of clusters and does not need the number of clusters to be specified upfront.\n",
        "\n",
        "**5. What are the advantages of DBSCAN over K-Means?**\n",
        "DBSCAN can detect clusters of arbitrary shapes, handles noise well, and doesn’t require the number of clusters to be specified beforehand, unlike K-Means which assumes spherical clusters and needs `k` as input.\n",
        "\n",
        "**6. When would you use Silhouette Score in clustering?**\n",
        "Silhouette Score is used to measure how well a point fits within its cluster compared to other clusters. It helps in evaluating and selecting the optimal number of clusters, especially when ground truth is not available.\n",
        "\n",
        "**7. What are the limitations of Hierarchical Clustering?**\n",
        "Hierarchical clustering is computationally intensive for large datasets, sensitive to noise, and does not allow reassignments once a merge or split has occurred. It also struggles with scalability.\n",
        "\n",
        "**8. Why is feature scaling important in clustering algorithms like K-Means?**\n",
        "Since K-Means relies on distance calculations, unscaled features can distort cluster formation. Feature scaling ensures all features contribute equally, preventing dominant features from skewing results.\n",
        "\n",
        "**9. How does DBSCAN identify noise points?**\n",
        "DBSCAN labels a point as noise if it has fewer than `min_samples` neighbors within the `eps` radius and is not part of any cluster. These points are outliers not assigned to any group.\n",
        "\n",
        "**10. Define inertia in the context of K-Means.**\n",
        "Inertia is the sum of squared distances between each point and its assigned cluster centroid. It measures the compactness of clusters and is used to evaluate the clustering performance.\n",
        "\n",
        "**11. What is the elbow method in K-Means clustering?**\n",
        "The elbow method involves plotting the inertia against different values of `k`. The “elbow” point, where the inertia starts to level off, is considered the optimal number of clusters.\n",
        "\n",
        "**12. Describe the concept of \"density\" in DBSCAN.**\n",
        "Density in DBSCAN refers to the number of data points within a given radius (`eps`) of a point. A high-density area forms a cluster, while low-density regions are treated as noise or boundaries.\n",
        "\n",
        "**13. Can hierarchical clustering be used on categorical data?**\n",
        "Yes, hierarchical clustering can be applied to categorical data, but it requires appropriate distance metrics like Hamming distance or Gower distance, as Euclidean distance is unsuitable.\n",
        "\n",
        "**14. What does a negative Silhouette Score indicate?**\n",
        "A negative Silhouette Score means that a sample is closer to another cluster than to the one it’s assigned to. This indicates potential misclassification and poor clustering structure.\n",
        "\n",
        "**15. Explain the term \"linkage criteria\" in hierarchical clustering.**\n",
        "Linkage criteria determine how the distance between clusters is calculated. Common methods include single linkage (minimum distance), complete linkage (maximum), and average linkage (mean distance).\n",
        "\n",
        "**16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?**\n",
        "K-Means assumes equal-sized, spherical clusters with similar densities. When clusters vary significantly in size or density, K-Means can misclassify points, leading to poor clustering results.\n",
        "\n",
        "**17. What are the core parameters in DBSCAN, and how do they influence clustering?**\n",
        "DBSCAN uses two main parameters: `eps` (radius) and `min_samples` (minimum number of points). These control the definition of a dense region. Larger `eps` or smaller `min_samples` can lead to fewer but larger clusters.\n",
        "\n",
        "**18. How does K-Means++ improve upon standard K-Means initialization?**\n",
        "K-Means++ selects initial centroids in a way that spreads them out, reducing the chance of poor initialization and improving convergence speed and clustering accuracy.\n",
        "\n",
        "**19. What is agglomerative clustering?**\n",
        "Agglomerative clustering is a type of hierarchical clustering that builds clusters in a bottom-up approach, starting with individual points and merging the closest pairs step-by-step until all points form one cluster.\n",
        "\n",
        "**20. What makes Silhouette Score a better metric than just inertia for model evaluation?**\n",
        "While inertia measures only compactness, Silhouette Score considers both cohesion (intra-cluster similarity) and separation (inter-cluster dissimilarity), offering a more holistic view of clustering quality.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gieTmt0Bjyla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical assignment"
      ],
      "metadata": {
        "id": "bXSBJoYQlWnK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlGvIXECi9Vv"
      },
      "outputs": [],
      "source": [
        "# Practical Assignment: Clustering Algorithms using Synthetic and Real Datasets\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_blobs, make_moons, make_circles, load_iris, load_wine, load_digits, load_breast_cancer\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# 1. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "labels = kmeans.fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title(\"K-Means Clustering (4 centers)\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels\n",
        "iris = load_iris()\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(iris.data)\n",
        "print(\"First 10 Agglomerative Clustering Labels on Iris:\", labels[:10])\n",
        "\n",
        "# 3. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot\n",
        "X, _ = make_moons(n_samples=300, noise=0.05)\n",
        "db = DBSCAN(eps=0.2, min_samples=5).fit(X)\n",
        "labels = db.labels_\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Spectral')\n",
        "plt.title(\"DBSCAN on make_moons\")\n",
        "plt.show()\n",
        "\n",
        "# 4. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster\n",
        "wine = load_wine()\n",
        "X_scaled = StandardScaler().fit_transform(wine.data)\n",
        "labels = KMeans(n_clusters=3, random_state=42).fit_predict(X_scaled)\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(\"Cluster Sizes in Wine Dataset:\", dict(zip(unique, counts)))\n",
        "\n",
        "# 5. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result\n",
        "X, _ = make_circles(n_samples=300, noise=0.05, factor=0.5)\n",
        "labels = DBSCAN(eps=0.2).fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='plasma')\n",
        "plt.title(\"DBSCAN on make_circles\")\n",
        "plt.show()\n",
        "\n",
        "# 6. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids\n",
        "data = load_breast_cancer()\n",
        "X_scaled = MinMaxScaler().fit_transform(data.data)\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "print(\"KMeans Cluster Centroids on Breast Cancer:\\n\", kmeans.cluster_centers_)\n",
        "\n",
        "# 7. Generate synthetic data using make_blobs with varying cluster std devs and cluster with DBSCAN\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)\n",
        "labels = DBSCAN(eps=1.5).fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(\"DBSCAN with Varying Cluster Standard Deviations\")\n",
        "plt.show()\n",
        "\n",
        "# 8. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means\n",
        "digits = load_digits()\n",
        "X_pca = PCA(n_components=2).fit_transform(digits.data)\n",
        "labels = KMeans(n_clusters=10, random_state=42).fit_predict(X_pca)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='nipy_spectral')\n",
        "plt.title(\"KMeans Clustering on Digits PCA\")\n",
        "plt.show()\n",
        "\n",
        "# 9. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "scores = [silhouette_score(X, KMeans(n_clusters=k).fit_predict(X)) for k in range(2, 6)]\n",
        "plt.bar(range(2, 6), scores)\n",
        "plt.title(\"Silhouette Scores for K = 2 to 5\")\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.show()\n",
        "\n",
        "# 10. Load the Iris dataset and use hierarchical clustering. Plot a dendrogram with average linkage\n",
        "linked = linkage(iris.data, method='average')\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linked, labels=iris.target)\n",
        "plt.title(\"Hierarchical Clustering - Average Linkage\")\n",
        "plt.show()\n",
        "\n",
        "# 11. Generate overlapping clusters using make_blobs, apply KMeans and visualize with decision boundaries\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=42)\n",
        "kmeans = KMeans(n_clusters=3).fit(X)\n",
        "labels = kmeans.predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='coolwarm')\n",
        "plt.title(\"KMeans with Overlapping Clusters\")\n",
        "plt.show()\n",
        "\n",
        "# 12. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results\n",
        "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(digits.data)\n",
        "labels = DBSCAN(eps=5).fit_predict(X_tsne)\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='Spectral')\n",
        "plt.title(\"DBSCAN on Digits (t-SNE)\")\n",
        "plt.show()\n",
        "\n",
        "# 13. Generate synthetic data and apply Agglomerative Clustering with complete linkage. Plot the result\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "labels = AgglomerativeClustering(linkage='complete', n_clusters=3).fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Accent')\n",
        "plt.title(\"Agglomerative Clustering (Complete Linkage)\")\n",
        "plt.show()\n",
        "\n",
        "# 14. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using KMeans\n",
        "X_scaled = StandardScaler().fit_transform(data.data)\n",
        "inertias = [KMeans(n_clusters=k).fit(X_scaled).inertia_ for k in range(2, 7)]\n",
        "plt.plot(range(2, 7), inertias, marker='o')\n",
        "plt.title(\"KMeans Inertia on Breast Cancer\")\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.show()\n",
        "\n",
        "# 15. Generate synthetic concentric circles and cluster using Agglomerative Clustering with single linkage\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "labels = AgglomerativeClustering(linkage='single', n_clusters=2).fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set2')\n",
        "plt.title(\"Agglomerative Clustering on make_circles\")\n",
        "plt.show()\n",
        "\n",
        "# 16. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)\n",
        "X_scaled = StandardScaler().fit_transform(wine.data)\n",
        "labels = DBSCAN(eps=1.5).fit_predict(X_scaled)\n",
        "clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"DBSCAN Clusters on Wine (excluding noise):\", clusters)\n",
        "\n",
        "# 17. Generate data and apply KMeans. Plot cluster centers on data points\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "kmeans = KMeans(n_clusters=3).fit(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='Paired')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')\n",
        "plt.title(\"KMeans Centers on make_blobs\")\n",
        "plt.show()\n",
        "\n",
        "# 18. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise\n",
        "X = StandardScaler().fit_transform(iris.data)\n",
        "labels = DBSCAN(eps=0.6, min_samples=5).fit_predict(X)\n",
        "noise_points = list(labels).count(-1)\n",
        "print(\"DBSCAN Noise Samples in Iris:\", noise_points)\n",
        "\n",
        "# 19. Generate make_moons, apply KMeans, and visualize clustering\n",
        "X, _ = make_moons(n_samples=300, noise=0.1)\n",
        "labels = KMeans(n_clusters=2).fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='spring')\n",
        "plt.title(\"KMeans on Non-Linearly Separable Data (make_moons)\")\n",
        "plt.show()\n",
        "\n",
        "# 20. Load Digits, PCA (3D), KMeans, and visualize\n",
        "X_pca = PCA(n_components=3).fit_transform(digits.data)\n",
        "labels = KMeans(n_clusters=10).fit_predict(X_pca)\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap='Spectral')\n",
        "ax.set_title(\"3D Clustering of Digits via PCA + KMeans\")\n",
        "plt.show()\n",
        "\n",
        "# 21. Generate blobs with 5 centers, apply KMeans, and evaluate using silhouette_score\n",
        "X, _ = make_blobs(n_samples=500, centers=5, random_state=42)\n",
        "labels = KMeans(n_clusters=5).fit_predict(X)\n",
        "score = silhouette_score(X, labels)\n",
        "print(\"Silhouette Score (5-center Blobs):\", score)\n",
        "\n",
        "# 22. Breast Cancer dataset, PCA, Agglomerative Clustering, 2D visualization\n",
        "X_pca = PCA(n_components=2).fit_transform(data.data)\n",
        "labels = AgglomerativeClustering(n_clusters=2).fit_predict(X_pca)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='cool')\n",
        "plt.title(\"Breast Cancer - PCA + Agglomerative\")\n",
        "plt.show()\n",
        "\n",
        "# 23. make_circles clustering: KMeans vs DBSCAN side-by-side\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "axes[0].scatter(X[:, 0], X[:, 1], c=KMeans(n_clusters=2).fit_predict(X))\n",
        "axes[0].set_title(\"KMeans\")\n",
        "axes[1].scatter(X[:, 0], X[:, 1], c=DBSCAN(eps=0.2).fit_predict(X))\n",
        "axes[1].set_title(\"DBSCAN\")\n",
        "plt.suptitle(\"make_circles: KMeans vs DBSCAN\")\n",
        "plt.show()\n",
        "\n",
        "# 24. Load Iris dataset and plot Silhouette Coefficient for each sample after KMeans clustering\n",
        "from sklearn.metrics import silhouette_samples\n",
        "X = StandardScaler().fit_transform(iris.data)\n",
        "labels = KMeans(n_clusters=3).fit_predict(X)\n",
        "sample_silhouette_values = silhouette_samples(X, labels)\n",
        "plt.bar(range(len(X)), sample_silhouette_values)\n",
        "plt.title(\"Silhouette Coefficient per Sample (Iris)\")\n",
        "plt.show()\n",
        "\n",
        "# 25. make_blobs + Agglomerative Clustering with 'average' linkage. Visualize clusters\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "labels = AgglomerativeClustering(n_clusters=3, linkage='average').fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Dark2')\n",
        "plt.title(\"Agglomerative Clustering (Average Linkage)\")\n",
        "plt.show()\n",
        "\n",
        "# 26. Wine dataset + KMeans + seaborn pairplot (first 4 features)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(wine.data[:, :4], columns=wine.feature_names[:4])\n",
        "df['cluster'] = KMeans(n_clusters=3).fit_predict(wine.data)\n",
        "sns.pairplot(df, hue='cluster')\n",
        "plt.suptitle(\"KMeans Cluster Assignment on Wine Data (First 4 Features)\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# 27. Generate noisy blobs, use DBSCAN to identify clusters and noise, print the count\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.5, random_state=42)\n",
        "labels = DBSCAN(eps=1.2).fit_predict(X)\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = list(labels).count(-1)\n",
        "print(\"DBSCAN: Clusters =\", n_clusters, \", Noise Points =\", n_noise)\n",
        "\n",
        "# 28. Digits dataset, reduce with t-SNE, apply Agglomerative Clustering, plot\n",
        "X_tsne = TSNE(n_components=2).fit_transform(digits.data)\n",
        "labels = AgglomerativeClustering(n_clusters=10).fit_predict(X_tsne)\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(\"Digits: Agglomerative Clustering on t-SNE\")\n",
        "plt.show()\n"
      ]
    }
  ]
}